{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexkagozi/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/var/folders/m9/m7xl4h0s4dxcq7r_gtrcb8m80000gn/T/ipykernel_25543/320270234.py:32: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = mpl.cm.get_cmap('coolwarm')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # \"jax\" or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_cv\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import librosa.display as lid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as ply\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "from itertools import cycle\n",
    "import soundfile as sf\n",
    "# Set interactive backend\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "cmap = mpl.cm.get_cmap('coolwarm')\n",
    "sns.set_theme(style=\"white\", palette=None)\n",
    "color_pal = ply.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "color_cycle = cycle(ply.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About YAMNet\n",
    "\n",
    "YAMNet is a pre-trained neural network that employs the MobileNetV1 depthwise-separable convolution architecture. It can use an audio waveform as input and make independent predictions for each of the 521 audio events from the AudioSet corpus.\n",
    "\n",
    "Internally, the model extracts \"frames\" from the audio signal and processes batches of these frames. This version of the model uses frames that are 0.96 second long and extracts one frame every 0.48 seconds .\n",
    "\n",
    "The model accepts a 1-D float32 Tensor or NumPy array containing a waveform of arbitrary length, represented as single-channel (mono) 16 kHz samples in the range [-1.0, +1.0].\n",
    "\n",
    "The model returns 3 outputs, including the class scores, embeddings (which you will use for transfer learning), and the log mel spectrogram.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Dataset\n",
    "DATASET_PATH = 'content/birdclef-2024'\n",
    "## To handle our settings  and configurations, let's create a class\n",
    "class Config:    \n",
    "    #Yamnet Model\n",
    "    sample_rate = 32000\n",
    "    preset = 'https://tfhub.dev/google/yamnet/1'\n",
    "    class_names = sorted(os.listdir(f'{DATASET_PATH}/train_audio/'))\n",
    "    num_classes = len(class_names)\n",
    "    class_labels = list(range(num_classes))\n",
    "    label2name = dict(zip(class_labels, class_names))\n",
    "    name2label = {v:k for k,v in label2name.items()}\n",
    "    # Define split ratios\n",
    "    train_ratio = 0.8  # 80% for training\n",
    "    val_ratio = 0.1    # 10% for validation\n",
    "    test_ratio = 0.1   # 10% for testing \n",
    "    batch_size = 32\n",
    "    audio_len = 1024\n",
    "    epochs = 10\n",
    "    AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content/birdclef-2024/train_audio/blrwar1/XC184748.ogg\n",
      "content/birdclef-2024/train_audio/whtkin2/XC797017.ogg\n",
      "content/birdclef-2024/train_audio/hoopoe/XC349675.ogg\n",
      "content/birdclef-2024/train_audio/grnsan/XC478932.ogg\n",
      "content/birdclef-2024/train_audio/tibfly3/XC645726.ogg\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'{DATASET_PATH}/train_metadata.csv')\n",
    "df['filepath'] = DATASET_PATH + '/train_audio/' + df.filename\n",
    "df['target'] = df.primary_label.map(Config.name2label)\n",
    "df['filename'] = df.filepath.map(lambda x: x.split('/')[-1])\n",
    "df['xc_id'] = df.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "## display a few rows of the dataframe\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "df.head(5)\n",
    "for row in df.head(5).iterrows():\n",
    "    print(row[1].filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert .ogg to .wav while maintaining directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to convert ogg to wav\n",
    "# Function to convert .ogg to .wav while maintaining directory structure\n",
    "\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "dataset_path = \"content/birdclef-2024\"\n",
    "train_audio_path = os.path.join(dataset_path, \"train_audio\")\n",
    "output_path = os.path.join(dataset_path, \"train_wav_audio\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "def convert_ogg_to_wav(input_path, output_path):\n",
    "    # Load the .ogg file\n",
    "    audio = AudioSegment.from_ogg(input_path)\n",
    "    # Export as .wav\n",
    "    audio.export(output_path, format=\"wav\")\n",
    "\n",
    "# Iterate over each row in the dataset\n",
    "# for index, row in df.iterrows():\n",
    "#     ogg_filepath = row[\"filepath\"]\n",
    "\n",
    "#     # Extract subdirectory name\n",
    "#     relative_path = os.path.relpath(ogg_filepath, train_audio_path)  # e.g., \"blrwar1/XC184748.ogg\"\n",
    "#     subdir = os.path.dirname(relative_path)  # e.g., \"blrwar1\"\n",
    "    \n",
    "#     # Construct output directory\n",
    "#     output_subdir = os.path.join(output_path, subdir)\n",
    "#     os.makedirs(output_subdir, exist_ok=True)  # Ensure subdirectory exists\n",
    "    \n",
    "#     # Construct the output file path\n",
    "#     filename = os.path.basename(ogg_filepath).replace(\".ogg\", \".wav\")\n",
    "#     wav_filepath = os.path.join(output_subdir, filename)\n",
    "    \n",
    "#     # Convert .ogg to .wav\n",
    "#     convert_ogg_to_wav(ogg_filepath, wav_filepath)\n",
    "#     print(f\"Converted: {ogg_filepath} -> {wav_filepath}\")\n",
    "# print(\"Conversion complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to load audio files, which will also be used later when working with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df['filepath'] = df.filepath.map(lambda x: x.replace('train_audio', 'train_wav_audio').replace('.ogg', '.wav')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio using librosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_16k_mono(filename):\n",
    "    \"\"\"\n",
    "    Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio using librosa.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Path to the WAV file.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Resampled audio data as a 1D numpy array.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    audio, _ = librosa.load(filename, sr=Config.sample_rate, mono=True)\n",
    "    return audio.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3737008e-06,  9.2573464e-07, -2.7166680e-06, ...,\n",
       "        6.2165782e-06, -1.4831312e-06, -3.4761615e-06], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test the function\n",
    "df.filepath[0]\n",
    "wav = load_wav_16k_mono(df.filepath[0])\n",
    "wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = plt.plot(wav)\n",
    "\n",
    "# Play the audio file.\n",
    "ipd.Audio(wav, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the audio files and retrieve embeddings\n",
    "Here you'll apply the load_wav_16k_mono and prepare the WAV data for the model.\n",
    "\n",
    "When extracting embeddings from the WAV data, you get an array of shape (N, 1024) where N is the number of frames that YAMNet found (one for every 0.48 seconds of audio).\n",
    "\n",
    "Your model will use each frame as one input. Therefore, you need to create a new column that has one frame per row. You also need to expand the labels and the fold column to proper reflect these new rows.\n",
    "\n",
    "The expanded fold column keeps the original values. You cannot mix frames because, when performing the splits, you might end up having parts of the same audio on different splits, which would make your validation and test steps less effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YamNet model\n",
    "yamnet_model = hub.load(Config.preset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a decoder parse files into embeddings🚀 \n",
    "\n",
    "**The build_decoder() function constructs a decoder that can process audio files into embeddings.\n",
    "It loads, normalizes, and converts the audio into embeddings.\n",
    "If with_labels=True, it also converts labels into one-hot vectors.\n",
    "The output is Yamnet Embeddings that can be used as input to CNNs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def build_decoder(with_labels=True, dim=1024):\n",
    "    \"\"\"\n",
    "    Builds a function to decode and preprocess audio files into embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    - with_labels (bool): Whether to return labels along with embeddings.\n",
    "    - dim (int): Target audio length (number of samples).\n",
    "    \n",
    "    Returns:\n",
    "    - Function to decode audio files (with or without labels).\n",
    "    \"\"\"\n",
    "\n",
    "    def get_audio(filepath):\n",
    "        \"\"\"Loads and decodes an audio file from a given filepath using librosa.\"\"\"\n",
    "        def _load_audio(filepath):\n",
    "            # Load the audio file using librosa\n",
    "            audio, _ = librosa.load(filepath.numpy().decode('utf-8'), sr=Config.sample_rate, mono=True)\n",
    "            return audio.astype(np.float32)\n",
    "        # Use tf.py_function to wrap the librosa call\n",
    "        audio = tf.py_function(_load_audio, [filepath], tf.float32)\n",
    "        audio.set_shape([None])  # Set shape to [None] since the length may vary\n",
    "        return audio\n",
    "\n",
    "    # def crop_or_pad(audio, target_len, pad_mode=\"constant\"):\n",
    "    #     \"\"\"Ensures the audio is of fixed length by either cropping or padding.\"\"\"\n",
    "        \n",
    "    #     audio_len = tf.shape(audio)[0]  # Get current length of audio\n",
    "    #     diff_len = abs(target_len - audio_len)  # Difference from target length\n",
    "        \n",
    "    #     audio = audio[:48000]\n",
    "    #     zero_padding = tf.zeros([48000] - tf.shape(wav), dtype=tf.float32)\n",
    "    #     audio = tf.concat([zero_padding, audio],0)\n",
    "\n",
    "    #     # if audio_len < target_len:\n",
    "    #     #     # If audio is shorter, pad it randomly on both sides\n",
    "    #     #     pad1 = tf.random.uniform([], maxval=diff_len, dtype=tf.int32)\n",
    "    #     #     pad2 = diff_len - pad1\n",
    "    #     #     audio = tf.pad(audio, paddings=[[pad1, pad2]], mode=pad_mode)\n",
    "\n",
    "    #     # elif audio_len > target_len:\n",
    "    #     #     # If audio is longer, randomly crop a section\n",
    "    #     #     idx = tf.random.uniform([], maxval=diff_len, dtype=tf.int32)\n",
    "    #     #     audio = audio[idx : (idx + target_len)]\n",
    "    #     return audio\n",
    "    #     # return tf.reshape(audio, [target_len])  # Ensure fixed shape\n",
    "    def crop_or_pad(audio, target_len, pad_mode=\"constant\"):\n",
    "        \"\"\"\n",
    "        Ensures the audio is of fixed length by either cropping or padding.\n",
    "        \n",
    "        Args:\n",
    "            audio (tf.Tensor): Input audio tensor of shape [None].\n",
    "            target_len (int): Target length of the audio.\n",
    "            pad_mode (str): Padding mode (e.g., \"constant\", \"reflect\").\n",
    "            \n",
    "        Returns:\n",
    "            tf.Tensor: Audio tensor of shape [target_len].\n",
    "        \"\"\"\n",
    "        audio_len = tf.shape(audio)[0]  # Get current length of audio\n",
    "        \n",
    "        if audio_len < target_len:\n",
    "            # If audio is shorter, pad it with zeros (or other padding mode)\n",
    "            padding_len = target_len - audio_len\n",
    "            pad1 = padding_len // 2  # Pad half on the left\n",
    "            pad2 = padding_len - pad1  # Pad the rest on the right\n",
    "            audio = tf.pad(audio, paddings=[[pad1, pad2]], mode=pad_mode)\n",
    "        elif audio_len > target_len:\n",
    "            # If audio is longer, crop the center portion\n",
    "            start = (audio_len - target_len) // 2\n",
    "            audio = audio[start : start + target_len]\n",
    "        else:\n",
    "            # If audio is already the target length, do nothing\n",
    "            pass\n",
    "        \n",
    "        # Ensure the output has the exact target length\n",
    "        audio = audio[:target_len]\n",
    "        return audio\n",
    "\n",
    "    def get_target(target):\n",
    "        \"\"\"Converts a label into a one-hot encoded vector.\"\"\"\n",
    "        target = tf.reshape(target, [1])  # Reshape to single element tensor\n",
    "        target = tf.cast(tf.one_hot(target, Config.num_classes), tf.float32)  # One-hot encoding\n",
    "        return tf.reshape(target, [Config.num_classes])  # Reshape to match the output format\n",
    "\n",
    "    def decode(path):\n",
    "        \"\"\"Processes an audio file into a spectrogram image.\"\"\"\n",
    "        # Load and preprocess the audio\n",
    "        audio = get_audio(path)\n",
    "        audio = crop_or_pad(audio, 48000)  # Ensure fixed length\n",
    "        \n",
    "        # Convert audio to YamNet embeddings\n",
    "        scores, embeddings, spectrogram = yamnet_model(audio)\n",
    "        embeddings = tf.reduce_mean(embeddings, axis=0)\n",
    "        embeddings.set_shape([dim])\n",
    "        return embeddings\n",
    "\n",
    "    def decode_with_labels(path, label):\n",
    "        \"\"\"Processes an audio file into a spectrogram and returns it with its label.\"\"\"\n",
    "        return decode(path), get_target(label)\n",
    "\n",
    "    return decode_with_labels if with_labels else decode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "def build_dataset(\n",
    "    paths, \n",
    "    labels=None, \n",
    "    batch_size=32,\n",
    "    decode_fn=None, \n",
    "    cache=True,\n",
    "    shuffle=2048\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a TensorFlow dataset pipeline for audio processing.\n",
    "\n",
    "    Args:\n",
    "        paths (list or tf.Tensor): List of file paths to audio files.\n",
    "        labels (list or tf.Tensor, optional): Corresponding labels for classification. Defaults to None.\n",
    "        batch_size (int, optional): Number of samples per batch. Defaults to 32.\n",
    "        decode_fn (function, optional): Function to decode audio files. Defaults to None.\n",
    "        cache (bool, optional): Whether to cache the dataset in memory. Defaults to True.\n",
    "        shuffle (int or bool, optional): Buffer size for shuffling. Set to False to disable shuffling. Defaults to 2048.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Preprocessed dataset ready for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use default decoder if none is provided\n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(with_labels=(labels is not None), dim=Config.audio_len)\n",
    "\n",
    "\n",
    "    # Set automatic tuning for dataset performance optimization\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # Create dataset from file paths (with or without labels)\n",
    "    slices = (paths,) if labels is None else (paths, labels)\n",
    "    print(f\"Labels: {labels}\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "\n",
    "    # Apply decoding function to process audio files\n",
    "    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n",
    "\n",
    "    # Cache dataset in memory to speed up subsequent iterations\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    # Shuffle dataset if required\n",
    "    if shuffle:\n",
    "        opt = tf.data.Options()\n",
    "        ds = ds.shuffle(shuffle, seed=seed)  # Shuffle with seed for reproducibility\n",
    "        opt.experimental_deterministic = False  # Improve performance by allowing non-deterministic order\n",
    "        ds = ds.with_options(opt)\n",
    "\n",
    "    # Batch dataset with a fixed size, ensuring even batch sizes\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # Prefetch data to improve training performance\n",
    "    ds = ds.prefetch(AUTO)\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 19567 | Num Valid: 2446 | Num Test: 2446\n"
     ]
    }
   ],
   "source": [
    "### Split the dataset to train, validation and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2)\n",
    "## spliet the validation set into validation and test sets\n",
    "valid_df, test_df = train_test_split(valid_df, test_size=0.5)\n",
    "print(f\"Num Train: {len(train_df)} | Num Valid: {len(valid_df)} | Num Test: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [167  45 105 ... 129  70 134]\n",
      "Labels: [143  62  78 ...   0 106  57]\n",
      "Labels: [106 132 167 ...  17 107  82]\n"
     ]
    }
   ],
   "source": [
    "# Prepare training dataset\n",
    "train_paths = train_df.filepath.values  # Extract file paths from training DataFrame\n",
    "train_labels = train_df.target.values   # Extract corresponding labels\n",
    "\n",
    "\n",
    "train_ds = build_dataset(\n",
    "    paths=train_paths, \n",
    "    labels=train_labels, \n",
    "    batch_size=1,\n",
    "    shuffle=True,  # Enable shuffling for training dataset\n",
    ")\n",
    "\n",
    "# Prepare validation dataset\n",
    "valid_paths = valid_df.filepath.values  # Extract file paths from validation DataFrame\n",
    "valid_labels = valid_df.target.values   # Extract corresponding labels\n",
    "\n",
    "valid_ds = build_dataset(\n",
    "    paths=valid_paths, \n",
    "    labels=valid_labels, \n",
    "    batch_size=1,\n",
    "    shuffle=False,  # No shuffling for validation to ensure consistency\n",
    ")\n",
    "\n",
    "# Prepare test dataset\n",
    "test_paths = test_df.filepath.values  # Extract file paths from test DataFrame\n",
    "test_labels = test_df.target.values   # Extract corresponding labels\n",
    "\n",
    "test_ds = build_dataset(\n",
    "    paths=test_paths, \n",
    "    labels=test_labels, \n",
    "    batch_size=1,\n",
    "    shuffle=False,  # No shuffling for test to ensure consistency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024) (1, 182)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 20:28:27.402749: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-04-02 20:28:27.428745: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "## Preview the shape of the training dataset\n",
    "for batch in train_ds.take(1):\n",
    "\tprint(batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">182</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">46,774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m182\u001b[0m)            │        \u001b[38;5;34m46,774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">702,902</span> (2.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m702,902\u001b[0m (2.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">702,902</span> (2.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m702,902\u001b[0m (2.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m19567/19567\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 7ms/step - accuracy: 0.0328 - loss: 4.9274 - val_accuracy: 0.0666 - val_loss: 4.4343\n",
      "Epoch 2/10\n",
      "\u001b[1m19567/19567\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - accuracy: 0.0467 - loss: 4.6400 - val_accuracy: 0.0446 - val_loss: 4.4930\n",
      "Epoch 3/10\n",
      "\u001b[1m12760/19567\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.0455 - loss: 4.6022"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     41\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# # Evaluate the model on the test dataset\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# test_loss, test_accuracy = model.evaluate(test_ds, verbose=1)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# print(f\"Test Loss: {test_loss}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# plt.title(\"Training and Validation Accuracy\")\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/AI/ArtificialIntelligence/BirdCLEF-2024/.venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Create a model using Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=input_shape, name=\"input_embedding\"),\n",
    "        layers.Dense(512, activation=\"relu\", name=\"dense_1\"),\n",
    "        layers.Dropout(0.5, name=\"dropout_1\"),\n",
    "        layers.Dense(256, activation=\"relu\", name=\"dense_2\"),\n",
    "        layers.Dropout(0.5, name=\"dropout_2\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\", name=\"output\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (1024,)  # YAMNet embeddings are 1024-dimensional\n",
    "num_classes = Config.num_classes  # Number of bird species\n",
    "\n",
    "# Create the model\n",
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# # Evaluate the model on the test dataset\n",
    "# test_loss, test_accuracy = model.evaluate(test_ds, verbose=1)\n",
    "# print(f\"Test Loss: {test_loss}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# # Save the model\n",
    "# model.save(\"bird_classification_model.h5\")\n",
    "# print(\"Model saved to bird_classification_model.h5\")\n",
    "\n",
    "# # Plot training and validation loss\n",
    "# plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "# plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Training and Validation Loss\")\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training and validation accuracy\n",
    "# plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "# plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Training and Validation Accuracy\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the model\n",
    "# Build the model\n",
    "model = build_model(num_classes=Config.num_classes)\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=100,\n",
    "    callbacks=[EarlyStopping(patience=3)],  # Stop early if validation loss doesn't improve\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "label_encoder = LabelEncoder()\n",
    "labels = df.primary_label\n",
    "file_paths = df.filepath\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(file_paths, encoded_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_or_pad(audio, target_len, pad_mode=\"constant\"):\n",
    "        \"\"\"\n",
    "        Ensures the audio is of fixed length by either cropping or padding.\n",
    "        \n",
    "        Args:\n",
    "            audio (tf.Tensor): Input audio tensor of shape [None].\n",
    "            target_len (int): Target length of the audio.\n",
    "            pad_mode (str): Padding mode (e.g., \"constant\", \"reflect\").\n",
    "            \n",
    "        Returns:\n",
    "            tf.Tensor: Audio tensor of shape [target_len].\n",
    "        \"\"\"\n",
    "        audio_len = tf.shape(audio)[0]  # Get current length of audio\n",
    "        \n",
    "        if audio_len < target_len:\n",
    "            # If audio is shorter, pad it with zeros (or other padding mode)\n",
    "            padding_len = target_len - audio_len\n",
    "            pad1 = padding_len // 2  # Pad half on the left\n",
    "            pad2 = padding_len - pad1  # Pad the rest on the right\n",
    "            audio = tf.pad(audio, paddings=[[pad1, pad2]], mode=pad_mode)\n",
    "        elif audio_len > target_len:\n",
    "            # If audio is longer, crop the center portion\n",
    "            start = (audio_len - target_len) // 2\n",
    "            audio = audio[start : start + target_len]\n",
    "        else:\n",
    "            # If audio is already the target length, do nothing\n",
    "            pass\n",
    "        \n",
    "        # Ensure the output has the exact target length\n",
    "        audio = audio[:target_len]\n",
    "        return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract embeddings\n",
    "def extract_embedding(file_path):\n",
    "    # Load and preprocess audio using librosa\n",
    "    ## Sr is the sampling rate of the audio file. \n",
    "    waveform, sample_rate = librosa.load(file_path, sr=16000, mono=True)\n",
    "    waveform = tf.convert_to_tensor(waveform, dtype=tf.float32)\n",
    "    # waveform = crop_or_pad(waveform, 48000)  # Ensure fixed length ## 48000 is the number of seconds in the audio file\n",
    "    # Extract embeddings\n",
    "    scores, embeddings, spectrogram = yamnet_model(waveform)\n",
    "    return embeddings.numpy().mean(axis=0)  # Average embeddings over time\n",
    "\n",
    "# Extract embeddings for training and testing data\n",
    "X_train_embeddings = np.array([extract_embedding(file_path) for file_path in X_train])\n",
    "X_test_embeddings = np.array([extract_embedding(file_path) for file_path in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the classifier model\n",
    "audio_embedding_model = models.Sequential([\n",
    "    layers.Input(shape=(X_train_embeddings.shape[1],)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "audio_embedding_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = audio_embedding_model.fit(X_train_embeddings, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test_embeddings, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = audio_embedding_model.evaluate(X_test_embeddings, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Predict on new data\n",
    "# def predict_bird(file_path):\n",
    "#     embedding = extract_embedding(file_path)\n",
    "#     prediction = audio_embedding_model.predict(embedding[np.newaxis, ...])\n",
    "#     predicted_label = label_encoder.inverse_transform([np.argmax(prediction)])\n",
    "#     return predicted_label[0]\n",
    "\n",
    "# Example usage\n",
    "# print(predict_bird('path_to_new_wav_file.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train_embeddings, y_train)\n",
    "accuracy = classifier.score(X_test_embeddings, y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try using another embedding extraction method\n",
    "from panns_inference import AudioTagging\n",
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PANNs model\n",
    "model = AudioTagging(checkpoint_path=None, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for your dataset\n",
    "X_train_embeddings = np.array([extract_embedding(file_path) for file_path in X_train])\n",
    "X_test_embeddings = np.array([extract_embedding(file_path) for file_path in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train_embeddings, y_train)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = classifier.score(X_test_embeddings, y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
