{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # \"jax\" or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_cv\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import librosa.display as lid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "cmap = mpl.cm.get_cmap('coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configuration\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    img_size = [128, 384]\n",
    "    batch_size = 64\n",
    "    duration = 15  # seconds\n",
    "    sample_rate = 32000\n",
    "    audio_len = duration * sample_rate\n",
    "    nfft = 2028\n",
    "    window = 2048\n",
    "    hop_length = audio_len // (img_size[1] - 1)\n",
    "    fmin = 20\n",
    "    fmax = 16000\n",
    "    epochs = 10\n",
    "    num_classes = len(os.listdir(\"content/birdclef-2024/train_audio/\"))\n",
    "    augment = True\n",
    "\n",
    "# Load dataset\n",
    "DATASET_PATH = 'content/birdclef-2024'\n",
    "df = pd.read_csv(f'{DATASET_PATH}/train_metadata.csv')\n",
    "df['filepath'] = DATASET_PATH + '/train_audio/' + df.filename\n",
    "df['target'] = df.primary_label.map({name: idx for idx, name in enumerate(sorted(os.listdir(f\"{DATASET_PATH}/train_audio/\")))})\n",
    "df['filename'] = df.filepath.map(lambda x: x.split('/')[-1])\n",
    "df['xc_id'] = df.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# Split dataset\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Audio processing functions\n",
    "def load_audio(filepath):\n",
    "    audio, sr = librosa.load(filepath, sr=CFG.sample_rate)\n",
    "    if len(audio) < CFG.audio_len:\n",
    "        padding = CFG.audio_len - len(audio)\n",
    "        audio = np.pad(audio, (0, padding), 'constant')\n",
    "    else:\n",
    "        audio = audio[:CFG.audio_len]\n",
    "    return audio, sr\n",
    "\n",
    "def get_spectrogram(audio):\n",
    "    spec = librosa.feature.melspectrogram(y=audio, sr=CFG.sample_rate, n_mels=256, n_fft=2048, hop_length=512, fmax=CFG.fmax, fmin=CFG.fmin)\n",
    "    spec = librosa.power_to_db(spec, ref=1.0)\n",
    "    spec = (spec - spec.min()) / (spec.max() - spec.min())\n",
    "    return spec\n",
    "\n",
    "# Custom Dataset\n",
    "class BirdCLEFDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio, _ = load_audio(row.filepath)\n",
    "        audio = audio[:CFG.audio_len]\n",
    "        spec = get_spectrogram(audio)\n",
    "        spec = np.repeat(spec[..., np.newaxis], 3, -1)  # Convert to 3-channel image\n",
    "        spec = torch.tensor(spec, dtype=torch.float32).permute(2, 0, 1)\n",
    "        label = torch.tensor(row.target, dtype=torch.long)\n",
    "        return spec, label\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = BirdCLEFDataset(train_df)\n",
    "valid_dataset = BirdCLEFDataset(valid_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False)\n",
    "\n",
    "# Model\n",
    "class EfficientNetV2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EfficientNetV2, self).__init__()\n",
    "        self.backbone = models.efficientnet_v2_s(pretrained=True)\n",
    "        self.backbone.classifier[1] = nn.Linear(self.backbone.classifier[1].in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "model = EfficientNetV2(CFG.num_classes)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(CFG.epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(valid_loader):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Valid Loss: {valid_loss/len(valid_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
