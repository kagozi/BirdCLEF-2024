{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_15384\\4144608217.py:29: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = mpl.cm.get_cmap('coolwarm')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # \"jax\" or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_cv\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import librosa.display as lid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Set interactive backend\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "cmap = mpl.cm.get_cmap('coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration ðŸ’¥ðŸ’¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'content/birdclef-2024'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the species name and construct a dictionary to hold their values ðŸ“ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = sorted(os.listdir(f\"{DATASET_PATH}/train_audio/\"))\n",
    "num_classes = len(class_names)\n",
    "class_labels = list(range(num_classes))\n",
    "label2name = dict(zip(class_labels, class_names))\n",
    "name2label = {v:k for k,v in label2name.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 182\n",
      "{0: 'asbfly', 1: 'ashdro1', 2: 'ashpri1', 3: 'ashwoo2', 4: 'asikoe2'}\n",
      "{'asbfly': 0, 'ashdro1': 1, 'ashpri1': 2, 'ashwoo2': 3, 'asikoe2': 4}\n"
     ]
    }
   ],
   "source": [
    "## Print out the first 5 items in the label2name and name2label dictionaries\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print({k: label2name[k] for k in list(label2name)[:5]})\n",
    "print({k: name2label[k] for k in list(name2label)[:5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataframe ðŸ”ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>target</th>\n",
       "      <th>xc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8424</th>\n",
       "      <td>crseag1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'flight call']</td>\n",
       "      <td>11.4081</td>\n",
       "      <td>107.4119</td>\n",
       "      <td>Spilornis cheela</td>\n",
       "      <td>Crested Serpent-Eagle</td>\n",
       "      <td>Jelle Scharringa</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.xeno-canto.org/665788</td>\n",
       "      <td>XC665788.ogg</td>\n",
       "      <td>content/birdclef-2024/train_audio/crseag1/XC66...</td>\n",
       "      <td>50</td>\n",
       "      <td>XC665788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24021</th>\n",
       "      <td>zitcis1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['male', 'song']</td>\n",
       "      <td>37.4128</td>\n",
       "      <td>-5.9160</td>\n",
       "      <td>Cisticola juncidis</td>\n",
       "      <td>Zitting Cisticola</td>\n",
       "      <td>JosÃ© Carlos Sires</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.xeno-canto.org/268680</td>\n",
       "      <td>XC268680.ogg</td>\n",
       "      <td>content/birdclef-2024/train_audio/zitcis1/XC26...</td>\n",
       "      <td>181</td>\n",
       "      <td>XC268680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15356</th>\n",
       "      <td>ingori1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['adult', 'male', 'song']</td>\n",
       "      <td>21.7177</td>\n",
       "      <td>79.3275</td>\n",
       "      <td>Oriolus kundoo</td>\n",
       "      <td>Indian Golden Oriole</td>\n",
       "      <td>Rajgopal Patil</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.xeno-canto.org/571750</td>\n",
       "      <td>XC571750.ogg</td>\n",
       "      <td>content/birdclef-2024/train_audio/ingori1/XC57...</td>\n",
       "      <td>88</td>\n",
       "      <td>XC571750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14861</th>\n",
       "      <td>houspa</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call']</td>\n",
       "      <td>37.1551</td>\n",
       "      <td>-7.6945</td>\n",
       "      <td>Passer domesticus</td>\n",
       "      <td>House Sparrow</td>\n",
       "      <td>Nelson ConceiÃ§Ã£o</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.xeno-canto.org/527803</td>\n",
       "      <td>XC527803.ogg</td>\n",
       "      <td>content/birdclef-2024/train_audio/houspa/XC527...</td>\n",
       "      <td>82</td>\n",
       "      <td>XC527803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6740</th>\n",
       "      <td>compea</td>\n",
       "      <td>[]</td>\n",
       "      <td>['song']</td>\n",
       "      <td>28.5057</td>\n",
       "      <td>77.2202</td>\n",
       "      <td>Pavo cristatus</td>\n",
       "      <td>Indian Peafowl</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.xeno-canto.org/116747</td>\n",
       "      <td>XC116747.ogg</td>\n",
       "      <td>content/birdclef-2024/train_audio/compea/XC116...</td>\n",
       "      <td>42</td>\n",
       "      <td>XC116747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      primary_label secondary_labels                       type  latitude  \\\n",
       "8424        crseag1               []    ['call', 'flight call']   11.4081   \n",
       "24021       zitcis1               []           ['male', 'song']   37.4128   \n",
       "15356       ingori1               []  ['adult', 'male', 'song']   21.7177   \n",
       "14861        houspa               []                   ['call']   37.1551   \n",
       "6740         compea               []                   ['song']   28.5057   \n",
       "\n",
       "       longitude     scientific_name            common_name  \\\n",
       "8424    107.4119    Spilornis cheela  Crested Serpent-Eagle   \n",
       "24021    -5.9160  Cisticola juncidis      Zitting Cisticola   \n",
       "15356    79.3275      Oriolus kundoo   Indian Golden Oriole   \n",
       "14861    -7.6945   Passer domesticus          House Sparrow   \n",
       "6740     77.2202      Pavo cristatus         Indian Peafowl   \n",
       "\n",
       "                  author                                            license  \\\n",
       "8424    Jelle Scharringa  Creative Commons Attribution-NonCommercial-Sha...   \n",
       "24021  JosÃ© Carlos Sires  Creative Commons Attribution-NonCommercial-Sha...   \n",
       "15356     Rajgopal Patil  Creative Commons Attribution-NonCommercial-Sha...   \n",
       "14861   Nelson ConceiÃ§Ã£o  Creative Commons Attribution-NonCommercial-Sha...   \n",
       "6740         Mike Nelson  Creative Commons Attribution-NonCommercial-Sha...   \n",
       "\n",
       "       rating                                url      filename  \\\n",
       "8424      4.0  https://www.xeno-canto.org/665788  XC665788.ogg   \n",
       "24021     4.0  https://www.xeno-canto.org/268680  XC268680.ogg   \n",
       "15356     4.0  https://www.xeno-canto.org/571750  XC571750.ogg   \n",
       "14861     5.0  https://www.xeno-canto.org/527803  XC527803.ogg   \n",
       "6740      5.0  https://www.xeno-canto.org/116747  XC116747.ogg   \n",
       "\n",
       "                                                filepath  target     xc_id  \n",
       "8424   content/birdclef-2024/train_audio/crseag1/XC66...      50  XC665788  \n",
       "24021  content/birdclef-2024/train_audio/zitcis1/XC26...     181  XC268680  \n",
       "15356  content/birdclef-2024/train_audio/ingori1/XC57...      88  XC571750  \n",
       "14861  content/birdclef-2024/train_audio/houspa/XC527...      82  XC527803  \n",
       "6740   content/birdclef-2024/train_audio/compea/XC116...      42  XC116747  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{DATASET_PATH}/train_metadata.csv')\n",
    "df['filepath'] = DATASET_PATH + '/train_audio/' + df.filename\n",
    "df['target'] = df.primary_label.map(name2label)\n",
    "df['filename'] = df.filepath.map(lambda x: x.split('/')[-1])\n",
    "df['xc_id'] = df.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "## display a few rows\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to retreive an audio file ðŸŽµ\n",
    "**librosa is a python package for music and audio analysis. It provides the building blocks necessary to create music information retrieval systems**\n",
    "[Documentation here](https://librosa.org/doc/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the audio as a waveform `y`\n",
    "# Store the sampling rate as `sr`\n",
    "def load_audio(filepath):\n",
    "    audio, sr = librosa.load(filepath)\n",
    "    return audio, sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the audio spectrogram ðŸŒŠ. \n",
    "**A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. When applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling rate of the audio signal (32 kHz)\n",
    "sample_rate = 32000\n",
    "\n",
    "# Define the maximum frequency to include in the spectrogram (16 kHz)\n",
    "fmax = 16000\n",
    "\n",
    "# Define the minimum frequency to include in the spectrogram (20 Hz)\n",
    "fmin = 20\n",
    "\n",
    "# Function to compute the Mel-spectrogram of an audio signal\n",
    "def get_spectrogram(audio):\n",
    "    # Compute the Mel-spectrogram\n",
    "    spec = librosa.feature.melspectrogram(\n",
    "        y=audio,  # Input audio signal\n",
    "        sr=sample_rate,  # Sampling rate of the audio\n",
    "        n_mels=256,  # Number of Mel bands (frequency bins)\n",
    "        n_fft=2048,  # Size of the FFT window (determines frequency resolution)\n",
    "        hop_length=512,  # Number of samples between successive frames (determines time resolution)\n",
    "        fmax=fmax,  # Maximum frequency to include in the spectrogram\n",
    "        fmin=fmin,  # Minimum frequency to include in the spectrogram\n",
    "    )\n",
    "\n",
    "    # Convert the power spectrogram to decibel (dB) scale\n",
    "    # This makes the values more perceptually meaningful\n",
    "    spec = librosa.power_to_db(spec, ref=1.0)  # ref=1.0 is the reference value for dB calculation\n",
    "\n",
    "    # Normalize the spectrogram to the range [0, 1]\n",
    "    min_ = spec.min()  # Minimum value in the spectrogram\n",
    "    max_ = spec.max()  # Maximum value in the spectrogram\n",
    "    if max_ != min_:  # Avoid division by zero if the spectrogram is constant\n",
    "        spec = (spec - min_) / (max_ - min_)  # Normalize using min-max scaling\n",
    "\n",
    "    # Return the normalized Mel-spectrogram\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a few audio files with spectograms and their associated df details âš¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 15\n",
    "audio_len = duration * sample_rate\n",
    "def display_audio(row):\n",
    "    caption = f'Id: {row.filename} | Name: {row.common_name} | Sci.Name: {row.scientific_name}'\n",
    "    \n",
    "    audio, sr = load_audio(row.filepath)\n",
    "    audio = audio[:audio_len]\n",
    "    spec = get_spectrogram(audio)\n",
    "    \n",
    "    # Audio output widget\n",
    "    audio_output = widgets.Output()\n",
    "    with audio_output:\n",
    "        display(ipd.Audio(audio, rate=sample_rate))\n",
    "    \n",
    "    # Plot output widget\n",
    "    plot_output = widgets.Output()\n",
    "    with plot_output:\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(12, 6), sharex=True, tight_layout=True)\n",
    "        fig.suptitle(caption)\n",
    "        \n",
    "        # Plot waveform\n",
    "        lid.waveshow(audio, sr=sample_rate, ax=ax[0], color='b')\n",
    "        \n",
    "        # Plot spectrogram\n",
    "        lid.specshow(spec, sr=sample_rate, hop_length=512, n_fft=2048,\n",
    "                     fmin=fmin, fmax=fmax, x_axis='time', y_axis='mel', \n",
    "                     cmap='coolwarm', ax=ax[1])\n",
    "        \n",
    "        ax[0].set_xlabel('')\n",
    "        plt.show()\n",
    "\n",
    "    # Display side-by-side\n",
    "    display(widgets.HBox([audio_output, plot_output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57f439e353d475eb055671106f5b4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c3ab6dbd7148a190d12eb840f2393f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdda9dd7240c4bf1909dd1c1d9b66f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Display a few audio samples\n",
    "for i in range(3):\n",
    "    display_audio(df.sample(1).iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a decoder parse files into spectrogramsðŸš€ \n",
    "\n",
    "**The build_decoder() function constructs a decoder that can process audio files into spectrograms.\n",
    "It loads, normalizes, and converts the audio into a Mel-spectrogram.\n",
    "If with_labels=True, it also converts labels into one-hot vectors.\n",
    "The output is an RGB-like spectrogram image that can be used as input to CNNs.**\n",
    "[Tensorflow Documentation here](https://www.tensorflow.org/io/api_docs/python/tfio/audio/spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image and audio parameters\n",
    "img_size = [128, 384]  # Spectrogram image size (height, width)\n",
    "batch_size = 64  # Batch size for training\n",
    "hop_length = audio_len // (img_size[1] - 1)  # Hop length for spectrogram computation\n",
    "nfft = 2028  # FFT window size for computing the spectrogram\n",
    "\n",
    "def build_decoder(with_labels=True, dim=1024):\n",
    "    \"\"\"\n",
    "    Builds a function to decode and preprocess audio files into spectrograms.\n",
    "    \n",
    "    Parameters:\n",
    "    - with_labels (bool): Whether to return labels along with spectrograms.\n",
    "    - dim (int): Target audio length (number of samples).\n",
    "    \n",
    "    Returns:\n",
    "    - Function to decode audio files (with or without labels).\n",
    "    \"\"\"\n",
    "\n",
    "    def get_audio(filepath):\n",
    "        \"\"\"Loads and decodes an audio file from a given filepath.\"\"\"\n",
    "        file_bytes = tf.io.read_file(filepath)  # Read the audio file as bytes\n",
    "        audio = tfio.audio.decode_vorbis(file_bytes)  # Decode .ogg Vorbis file\n",
    "        audio = tf.cast(audio, tf.float32)  # Convert to float32\n",
    "\n",
    "        # Convert stereo to mono by selecting only one channel\n",
    "        if tf.shape(audio)[1] > 1:\n",
    "            audio = audio[..., 0:1]\n",
    "        audio = tf.squeeze(audio, axis=-1)  # Remove redundant dimensions\n",
    "        return audio\n",
    "\n",
    "    def crop_or_pad(audio, target_len, pad_mode=\"constant\"):\n",
    "        \"\"\"Ensures the audio is of fixed length by either cropping or padding.\"\"\"\n",
    "        audio_len = tf.shape(audio)[0]  # Get current length of audio\n",
    "        diff_len = abs(target_len - audio_len)  # Difference from target length\n",
    "\n",
    "        if audio_len < target_len:\n",
    "            # If audio is shorter, pad it randomly on both sides\n",
    "            pad1 = tf.random.uniform([], maxval=diff_len, dtype=tf.int32)\n",
    "            pad2 = diff_len - pad1\n",
    "            audio = tf.pad(audio, paddings=[[pad1, pad2]], mode=pad_mode)\n",
    "\n",
    "        elif audio_len > target_len:\n",
    "            # If audio is longer, randomly crop a section\n",
    "            idx = tf.random.uniform([], maxval=diff_len, dtype=tf.int32)\n",
    "            audio = audio[idx : (idx + target_len)]\n",
    "\n",
    "        return tf.reshape(audio, [target_len])  # Ensure fixed shape\n",
    "\n",
    "    def apply_preproc(spec):\n",
    "        \"\"\"Applies standardization and normalization to the spectrogram.\"\"\"\n",
    "        # Standardization: Zero mean and unit variance\n",
    "        mean = tf.math.reduce_mean(spec)\n",
    "        std = tf.math.reduce_std(spec)\n",
    "        spec = tf.where(tf.math.equal(std, 0), spec - mean, (spec - mean) / std)\n",
    "\n",
    "        # Min-Max Normalization: Scale values between 0 and 1\n",
    "        min_val = tf.math.reduce_min(spec)\n",
    "        max_val = tf.math.reduce_max(spec)\n",
    "        spec = tf.where(\n",
    "            tf.math.equal(max_val - min_val, 0), \n",
    "            spec - min_val, \n",
    "            (spec - min_val) / (max_val - min_val)\n",
    "        )\n",
    "\n",
    "        return spec\n",
    "\n",
    "    def get_target(target):\n",
    "        \"\"\"Converts a label into a one-hot encoded vector.\"\"\"\n",
    "        target = tf.reshape(target, [1])  # Reshape to single element tensor\n",
    "        target = tf.cast(tf.one_hot(target, num_classes), tf.float32)  # One-hot encoding\n",
    "        return tf.reshape(target, [num_classes])  # Reshape to match the output format\n",
    "\n",
    "    def decode(path):\n",
    "        \"\"\"Processes an audio file into a spectrogram image.\"\"\"\n",
    "        # Load and preprocess the audio\n",
    "        audio = get_audio(path)\n",
    "        audio = crop_or_pad(audio, dim)  # Ensure fixed length\n",
    "        \n",
    "        # Convert audio to a Mel-spectrogram\n",
    "        spec = keras.layers.MelSpectrogram(\n",
    "            num_mel_bins=img_size[0],  # Number of Mel frequency bins (height of image)\n",
    "            fft_length=nfft,  # FFT window size\n",
    "            sequence_stride=hop_length,  # Step size between spectrogram columns\n",
    "            sampling_rate=sample_rate,  # Sample rate of audio\n",
    "        )(audio)\n",
    "\n",
    "        spec = apply_preproc(spec)  # Apply normalization and standardization\n",
    "        \n",
    "        # Convert spectrogram into a 3-channel image (for compatibility with CNNs)\n",
    "        spec = tf.tile(spec[..., None], [1, 1, 3])  # Repeat values along the last axis\n",
    "        return tf.reshape(spec, [*img_size, 3])  # Reshape to (height, width, 3)\n",
    "\n",
    "    def decode_with_labels(path, label):\n",
    "        \"\"\"Processes an audio file into a spectrogram and returns it with its label.\"\"\"\n",
    "        return decode(path), get_target(label)\n",
    "\n",
    "    return decode_with_labels if with_labels else decode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation â™»\n",
    "##### augmentation involves applying a variety of transformations to the original dataset, generating new samples that are similar but not identical to the original data. Common augmentations include rotation, flipping, scaling, changes in brightness and contrast, color space adjustments, and geometric transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_augmenter():\n",
    "    \"\"\"\n",
    "    Creates an augmentation pipeline for spectrogram images.\n",
    "    Uses MixUp, time masking, and frequency masking to improve model generalization.\n",
    "    \n",
    "    Returns:\n",
    "        A function that applies random augmentations to images and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a list of augmentation techniques to apply\n",
    "    augmenters = [\n",
    "        keras_cv.layers.MixUp(alpha=0.4),  # MixUp augmentation for blending two images\n",
    "        keras_cv.layers.RandomCutout(\n",
    "            height_factor=(1.0, 1.0), width_factor=(0.06, 0.12)\n",
    "        ),  # Time-masking: Randomly removes sections along the time axis\n",
    "        keras_cv.layers.RandomCutout(\n",
    "            height_factor=(0.06, 0.1), width_factor=(1.0, 1.0)\n",
    "        ),  # Frequency-masking: Randomly removes sections along the frequency axis\n",
    "    ]\n",
    "\n",
    "    def augment(img, label):\n",
    "        \"\"\"\n",
    "        Applies the augmentation pipeline to an image-label pair.\n",
    "\n",
    "        Args:\n",
    "            img (tf.Tensor): Input spectrogram image.\n",
    "            label (tf.Tensor): Corresponding label for the image.\n",
    "\n",
    "        Returns:\n",
    "            Augmented image and label.\n",
    "        \"\"\"\n",
    "\n",
    "        # Wrap image and label in a dictionary for compatibility with keras_cv augmenters\n",
    "        data = {\"images\": img, \"labels\": label}\n",
    "\n",
    "        # Apply augmentations with a 35% probability for each augmenter\n",
    "        for augmenter in augmenters:\n",
    "            if tf.random.uniform([]) < 0.35:\n",
    "                data = augmenter(data, training=True)\n",
    "\n",
    "        # Extract and return augmented image and label\n",
    "        return data[\"images\"], data[\"labels\"]\n",
    "\n",
    "    return augment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dataset for training ðŸ’°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "def build_dataset(\n",
    "    paths, \n",
    "    labels=None, \n",
    "    batch_size=32,\n",
    "    decode_fn=None, \n",
    "    augment_fn=None, \n",
    "    cache=True,\n",
    "    augment=False, \n",
    "    shuffle=2048\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a TensorFlow dataset pipeline for audio processing.\n",
    "\n",
    "    Args:\n",
    "        paths (list or tf.Tensor): List of file paths to audio files.\n",
    "        labels (list or tf.Tensor, optional): Corresponding labels for classification. Defaults to None.\n",
    "        batch_size (int, optional): Number of samples per batch. Defaults to 32.\n",
    "        decode_fn (function, optional): Function to decode audio files. Defaults to None.\n",
    "        augment_fn (function, optional): Function to apply augmentations. Defaults to None.\n",
    "        cache (bool, optional): Whether to cache the dataset in memory. Defaults to True.\n",
    "        augment (bool, optional): Whether to apply data augmentation. Defaults to False.\n",
    "        shuffle (int or bool, optional): Buffer size for shuffling. Set to False to disable shuffling. Defaults to 2048.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Preprocessed dataset ready for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use default decoder if none is provided\n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(with_labels=(labels is not None), dim=audio_len)\n",
    "\n",
    "    # Use default augmentation function if none is provided\n",
    "    if augment_fn is None:\n",
    "        augment_fn = build_augmenter()\n",
    "\n",
    "    # Set automatic tuning for dataset performance optimization\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # Create dataset from file paths (with or without labels)\n",
    "    slices = (paths,) if labels is None else (paths, labels)\n",
    "    print(f\"Labels: {labels}\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "\n",
    "    # Apply decoding function to process audio files\n",
    "    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n",
    "\n",
    "    # Cache dataset in memory to speed up subsequent iterations\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    # Shuffle dataset if required\n",
    "    if shuffle:\n",
    "        opt = tf.data.Options()\n",
    "        ds = ds.shuffle(shuffle, seed=seed)  # Shuffle with seed for reproducibility\n",
    "        opt.experimental_deterministic = False  # Improve performance by allowing non-deterministic order\n",
    "        ds = ds.with_options(opt)\n",
    "\n",
    "    # Batch dataset with a fixed size, ensuring even batch sizes\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    # Apply augmentation if enabled\n",
    "    if augment:\n",
    "        ds = ds.map(augment_fn, num_parallel_calls=AUTO)\n",
    "\n",
    "    # Prefetch data to improve training performance\n",
    "    ds = ds.prefetch(AUTO)\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset to a test and train set ðŸš‚\n",
    "***We used a test size of 0.2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Train: 19567 | Num Valid: 4892\n"
     ]
    }
   ],
   "source": [
    "## Split the dataset into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "print(f\"Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training dataset\n",
    "train_paths = train_df.filepath.values  # Extract file paths from training DataFrame\n",
    "train_labels = train_df.target.values   # Extract corresponding labels\n",
    "\n",
    "train_ds = build_dataset(\n",
    "    paths=train_paths, \n",
    "    labels=train_labels, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # Enable shuffling for training dataset\n",
    "    augment=True  # Apply augmentation for training dataset\n",
    ")\n",
    "\n",
    "# Prepare validation dataset\n",
    "valid_paths = valid_df.filepath.values  # Extract file paths from validation DataFrame\n",
    "valid_labels = valid_df.target.values   # Extract corresponding labels\n",
    "\n",
    "valid_ds = build_dataset(\n",
    "    paths=valid_paths, \n",
    "    labels=valid_labels, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # No shuffling for validation to ensure consistency\n",
    "    augment=False  # No augmentation for validation dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
