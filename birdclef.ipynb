{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_9120\\2577343782.py:20: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap = mpl.cm.get_cmap('coolwarm')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch  \n",
    "import torchaudio\n",
    "import torchaudio.transforms as T  # For audio transformations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import librosa.display as lid  # librosa's display module\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "cmap = mpl.cm.get_cmap('coolwarm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Desktop\\CompScience\\ArtificialIntelligence\\BirdCLEF-2024\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\Desktop\\CompScience\\ArtificialIntelligence\\BirdCLEF-2024\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to C:\\Users\\PC/.cache\\torch\\hub\\checkpoints\\efficientnet_v2_s-dd5fe13b.pth\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 37.4M/82.7M [01:05<02:57, 268kB/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configuration\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    img_size = [128, 384]\n",
    "    batch_size = 64\n",
    "    duration = 15  # seconds\n",
    "    sample_rate = 32000\n",
    "    audio_len = duration * sample_rate\n",
    "    nfft = 2028\n",
    "    window = 2048\n",
    "    hop_length = audio_len // (img_size[1] - 1)\n",
    "    fmin = 20\n",
    "    fmax = 16000\n",
    "    epochs = 10\n",
    "    num_classes = len(os.listdir(\"content/birdclef-2024/train_audio/\"))\n",
    "    augment = True\n",
    "\n",
    "# Load dataset\n",
    "DATASET_PATH = 'content/birdclef-2024'\n",
    "df = pd.read_csv(f'{DATASET_PATH}/train_metadata.csv')\n",
    "df['filepath'] = DATASET_PATH + '/train_audio/' + df.filename\n",
    "df['target'] = df.primary_label.map({name: idx for idx, name in enumerate(sorted(os.listdir(f\"{DATASET_PATH}/train_audio/\")))})\n",
    "df['filename'] = df.filepath.map(lambda x: x.split('/')[-1])\n",
    "df['xc_id'] = df.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "# Split dataset\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Audio processing functions\n",
    "def load_audio(filepath):\n",
    "    audio, sr = librosa.load(filepath, sr=CFG.sample_rate)\n",
    "    return audio, sr\n",
    "\n",
    "def get_spectrogram(audio):\n",
    "    spec = librosa.feature.melspectrogram(y=audio, sr=CFG.sample_rate, n_mels=256, n_fft=2048, hop_length=512, fmax=CFG.fmax, fmin=CFG.fmin)\n",
    "    spec = librosa.power_to_db(spec, ref=1.0)\n",
    "    spec = (spec - spec.min()) / (spec.max() - spec.min())\n",
    "    return spec\n",
    "\n",
    "# Custom Dataset\n",
    "class BirdCLEFDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio, _ = load_audio(row.filepath)\n",
    "        audio = audio[:CFG.audio_len]\n",
    "        spec = get_spectrogram(audio)\n",
    "        spec = np.repeat(spec[..., np.newaxis], 3, -1)  # Convert to 3-channel image\n",
    "        spec = torch.tensor(spec, dtype=torch.float32).permute(2, 0, 1)\n",
    "        label = torch.tensor(row.target, dtype=torch.long)\n",
    "        return spec, label\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = BirdCLEFDataset(train_df)\n",
    "valid_dataset = BirdCLEFDataset(valid_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False)\n",
    "\n",
    "# Model\n",
    "class EfficientNetV2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EfficientNetV2, self).__init__()\n",
    "        self.backbone = models.efficientnet_v2_s(pretrained=True)\n",
    "        self.backbone.classifier[1] = nn.Linear(self.backbone.classifier[1].in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "model = EfficientNetV2(CFG.num_classes)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(CFG.epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader)}\")\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(valid_loader):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Valid Loss: {valid_loss/len(valid_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
